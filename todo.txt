- scraper 
create class to store new data - inherit from it for tweet vs news site etc

- analyzer
use worker pool to chew queue
process data based on class type of data taken off queue - ie tweet vs site

- general
allow access to each module's necessary configs (do parsing of each section and pass in the result to the run function)
logging

dependencies:
TextBlob
newspaper3k